{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d6932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1+cpu\n",
      "torchvision version: 0.19.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# output_dir = f\"./vit_outputs/exp_{timestamp}\"\n",
    "# os.makedirs(f\"./vit_outputs/exp_{timestamp}\", exist_ok=True)\n",
    "# os.makedirs(f\"./vit_outputs/exp_{timestamp}/models\", exist_ok=True)\n",
    "# os.makedirs(f\"./vit_outputs/exp_{timestamp}/plots\", exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and datasets\n",
    "import splitfolders\n",
    "import os\n",
    "\n",
    "path = '/leonardo/home/userexternal/anagi000/orig_dataset/skin_disease_data_crops' #'D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train'\n",
    "print(os.listdir(path))\n",
    "\n",
    "\n",
    "splitfolders.ratio(path, output=\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split\", seed=1337, ratio=(.6, .2, .2))\n",
    "\n",
    "\n",
    "#splitfolders.ratio(path, output=\"D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train_1000/mix_data_split\", seed=1337, ratio=(.6, .2, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/train\"\n",
    "test_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f4684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "destination_folder=\"/leonardo/home/userexternal/anagi000/orig_dataset/skin_disease_data_crops\"\n",
    "base_folder = Path(destination_folder)\n",
    "\n",
    "for subfolder in base_folder.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        file_count = sum(1 for f in subfolder.iterdir() if f.is_file())\n",
    "        print(f\"{subfolder.name}: {file_count} files\")\n",
    "\n",
    "\n",
    "# Split the data into training, validation and datasets\n",
    "# import splitfolders\n",
    "# import os\n",
    "\n",
    "# path = destination_folder\n",
    "# print(os.listdir(path))\n",
    "# splitfolders.ratio(path, output=\"./mix_data_split_Skin_Disease_Training_Data_Random_Sampling_from_scratch_exp_3_classes\", seed=435, ratio=(.6, .2, .2))\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Path to your main dataset folder\n",
    "main_folder = '/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split'  # <-- change this\n",
    "\n",
    "# Subfolders\n",
    "splits = ['train', 'test', 'val']\n",
    "\n",
    "# Dictionary to hold the results\n",
    "summary = {}\n",
    "\n",
    "for split in splits:\n",
    "    split_path = os.path.join(main_folder, split)\n",
    "    total_files = 0\n",
    "    summary[split] = {}\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(split_path):\n",
    "        print(f\"Directory not found: {split_path}\")\n",
    "        continue\n",
    "\n",
    "    for disease in os.listdir(split_path):\n",
    "        disease_path = os.path.join(split_path, disease)\n",
    "        \n",
    "        if not os.path.isdir(disease_path):\n",
    "            continue\n",
    "        \n",
    "        # Count files in disease folder\n",
    "        num_files = len([\n",
    "            f for f in os.listdir(disease_path)\n",
    "            if os.path.isfile(os.path.join(disease_path, f))\n",
    "        ])\n",
    "        \n",
    "        summary[split][disease] = num_files\n",
    "        total_files += num_files\n",
    "\n",
    "    print(f\"\\n{split.upper()} - Total files: {total_files}\")\n",
    "    for disease, count in summary[split].items():\n",
    "        print(f\"  {disease}: {count} files\")\n",
    "\n",
    "\n",
    "# Setup directory paths to train and test images\n",
    "train_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/train\"\n",
    "test_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/val\"\n",
    "\n",
    "\n",
    "# Create image size (from Table 3 in the ViT paper)\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "from going_modular import data_setup\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names\n",
    "\n",
    "\n",
    "from going_modular import data_setup\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names\n",
    "\n",
    "\n",
    "class_names\n",
    "\n",
    "\n",
    "from going_modular.helper_functions import set_seeds\n",
    "set_seeds()\n",
    "\n",
    "\n",
    "# 1. Get pretrained weights for ViT-Base\n",
    "# Get the pretrained weights for ViT-Base trained on ImageNet-1k from\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "#for parameter in pretrained_vit.parameters():\n",
    "#    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
    "#set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "\n",
    "#Original\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)\n",
    "\n",
    "\n",
    "\n",
    "# Setup dataloaders\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32)\n",
    "\n",
    "from going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "#optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "#                             lr=1e-3)\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-5)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "#set_seeds()\n",
    "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=100,\n",
    "                                      device=device)\n",
    "\n",
    "\n",
    "# Plot the loss curves\n",
    "from going_modular.helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(pretrained_vit_results, save_path=output_dir+\"/plots/loss_accuracy_curve.png\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "from going_modular import utils\n",
    "\n",
    "utils.save_model(model=pretrained_vit,\n",
    "                 target_dir=output_dir+\"/models\",\n",
    "                 model_name=\"Pretrained_vit__patches_sliding_window_algorithm_all_classes_scratch_training_mix_data_split_Skin_Disease_Training_Data_Random_Sampling.pth\")\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_vit_model_size = Path(output_dir+\"/models/Pretrained_vit__patches_sliding_window_algorithm_all_classes_scratch_training_mix_data_split_Skin_Disease_Training_Data_Random_Sampling.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cf341",
   "metadata": {},
   "source": [
    "# CONFORMAL PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abf6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the same device used during training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation and datasets\n",
    "# import splitfolders\n",
    "# import os\n",
    "\n",
    "# path = '/leonardo/home/userexternal/anagi000/orig_dataset/skin_disease_data_crops' #'D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train'\n",
    "# print(os.listdir(path))\n",
    "\n",
    "\n",
    "# splitfolders.ratio(path, output=\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split\", seed=1337, ratio=(0.5, 0.5), \n",
    "#                    group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2597a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crops_fair_skin_esantema-maculo-papuloso', 'crops_fair_skin_esantema-morbilliforme', 'crops_fair_skin_esantema-polimorfo-like', 'crops_fair_skin_esantema-virale', 'crops_fair_skin_esantema_iatrogeno_farmaco_indotta', 'crops_fair_skin_orticaria', 'crops_fair_skin_pediculosi', 'crops_fair_skin_scabbia', 'crops_fair_skin_varicella']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 327362 files [35:09, 155.16 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation and datasets\n",
    "import splitfolders\n",
    "import os\n",
    "\n",
    "path = 'D:\\VSCode_Projects\\Thesis_Project\\skin_disease_data' #'D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train'\n",
    "print(os.listdir(path))\n",
    "\n",
    "\n",
    "splitfolders.ratio(path, output=\"./ViT_Batch_Job/training_data_split_Conformal_Prediction\", seed=1337, ratio=(.6, .2, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation and datasets\n",
    "# import splitfolders\n",
    "# import os\n",
    "\n",
    "# path = 'D:\\VSCode_Projects\\Thesis_Project\\skin_disease_data' #'D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train'\n",
    "# print(os.listdir(path))\n",
    "\n",
    "\n",
    "# splitfolders.ratio(path, output=\"./ViT_Batch_Job/training_data_split_Conformal_Prediction\", seed=1337, ratio=(0.5, 0.5), \n",
    "#                    group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5fccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c4796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabe1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"D:/VSCode_Projects/Thesis_Project/Experiments/ViT_Experiment_9-7-2025\") / \"models\" / \"model_best_epoch_92_acc_0.9860.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation and datasets\n",
    "# import splitfolders\n",
    "# import os\n",
    "\n",
    "# path = '/leonardo/home/userexternal/anagi000/orig_dataset/skin_disease_data_crops' #'D:\\VSCode_Projects\\Thesis_Project\\Crops_Fair_Skin_Train'\n",
    "# print(os.listdir(path))\n",
    "\n",
    "\n",
    "# splitfolders.ratio(path, output=\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split\", seed=1337, ratio=(0.5, 0.5), \n",
    "#                    group_prefix=None)\n",
    "\n",
    "\n",
    "# import splitfolders\n",
    "\n",
    "# # Path to the current test folder\n",
    "# test_path = '/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/test'\n",
    "\n",
    "# # Split test into calibration (50%) and test (50%)\n",
    "# splitfolders.ratio(\n",
    "#     input=test_path,\n",
    "#     output='/leonardo/home/userexternal/anagi000/Project/swin_transformer/test_split_for_conformal',\n",
    "#     seed=2025,\n",
    "#     ratio=(.5, .5),\n",
    "#     group_prefix=None  # ensures it's by files, not grouped by name\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b348b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "val_dir = r\"./ViT_Batch_Job/training_data_split_Conformal_Prediction/val\"\n",
    "test_dir = r\"./ViT_Batch_Job/training_data_split_Conformal_Prediction/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da97c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN - Total files: 196412\n",
      "  crops_fair_skin_esantema-maculo-papuloso: 7005 files\n",
      "  crops_fair_skin_esantema-morbilliforme: 3546 files\n",
      "  crops_fair_skin_esantema-polimorfo-like: 7194 files\n",
      "  crops_fair_skin_esantema-virale: 50746 files\n",
      "  crops_fair_skin_esantema_iatrogeno_farmaco_indotta: 16510 files\n",
      "  crops_fair_skin_orticaria: 80373 files\n",
      "  crops_fair_skin_pediculosi: 5982 files\n",
      "  crops_fair_skin_scabbia: 19430 files\n",
      "  crops_fair_skin_varicella: 5626 files\n",
      "\n",
      "TEST - Total files: 65481\n",
      "  crops_fair_skin_esantema-maculo-papuloso: 2336 files\n",
      "  crops_fair_skin_esantema-morbilliforme: 1182 files\n",
      "  crops_fair_skin_esantema-polimorfo-like: 2399 files\n",
      "  crops_fair_skin_esantema-virale: 16917 files\n",
      "  crops_fair_skin_esantema_iatrogeno_farmaco_indotta: 5505 files\n",
      "  crops_fair_skin_orticaria: 26792 files\n",
      "  crops_fair_skin_pediculosi: 1995 files\n",
      "  crops_fair_skin_scabbia: 6478 files\n",
      "  crops_fair_skin_varicella: 1877 files\n",
      "\n",
      "VAL - Total files: 65469\n",
      "  crops_fair_skin_esantema-maculo-papuloso: 2335 files\n",
      "  crops_fair_skin_esantema-morbilliforme: 1182 files\n",
      "  crops_fair_skin_esantema-polimorfo-like: 2398 files\n",
      "  crops_fair_skin_esantema-virale: 16915 files\n",
      "  crops_fair_skin_esantema_iatrogeno_farmaco_indotta: 5503 files\n",
      "  crops_fair_skin_orticaria: 26791 files\n",
      "  crops_fair_skin_pediculosi: 1994 files\n",
      "  crops_fair_skin_scabbia: 6476 files\n",
      "  crops_fair_skin_varicella: 1875 files\n",
      "Manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# destination_folder=\"/leonardo/home/userexternal/anagi000/orig_dataset/skin_disease_data_crops\"\n",
    "# base_folder = Path(destination_folder)\n",
    "\n",
    "# for subfolder in base_folder.iterdir():\n",
    "#     if subfolder.is_dir():\n",
    "#         file_count = sum(1 for f in subfolder.iterdir() if f.is_file())\n",
    "#         print(f\"{subfolder.name}: {file_count} files\")\n",
    "\n",
    "# Split the data into training, validation and datasets\n",
    "# import splitfolders\n",
    "# import os\n",
    "# path = destination_folder\n",
    "# print(os.listdir(path))\n",
    "# splitfolders.ratio(path, output=\"./mix_data_split_Skin_Disease_Training_Data_Random_Sampling_from_scratch_exp_3_classes\", seed=435, ratio=(.6, .2, .2))\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Path to your main dataset folder\n",
    "main_folder = './ViT_Batch_Job/training_data_split_Conformal_Prediction'  # <-- change this\n",
    "\n",
    "# Subfolders\n",
    "splits = ['train', 'test', 'val']\n",
    "\n",
    "# Dictionary to hold the results\n",
    "summary = {}\n",
    "\n",
    "for split in splits:\n",
    "    split_path = os.path.join(main_folder, split)\n",
    "    total_files = 0\n",
    "    summary[split] = {}\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(split_path):\n",
    "        print(f\"Directory not found: {split_path}\")\n",
    "        continue\n",
    "\n",
    "    for disease in os.listdir(split_path):\n",
    "        disease_path = os.path.join(split_path, disease)\n",
    "        \n",
    "        if not os.path.isdir(disease_path):\n",
    "            continue\n",
    "        \n",
    "        # Count files in disease folder\n",
    "        num_files = len([\n",
    "            f for f in os.listdir(disease_path)\n",
    "            if os.path.isfile(os.path.join(disease_path, f))\n",
    "        ])\n",
    "        \n",
    "        summary[split][disease] = num_files\n",
    "        total_files += num_files\n",
    "\n",
    "    print(f\"\\n{split.upper()} - Total files: {total_files}\")\n",
    "    for disease, count in summary[split].items():\n",
    "        print(f\"  {disease}: {count} files\")\n",
    "\n",
    "\n",
    "# Setup directory paths to train and test images\n",
    "# train_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/train\"\n",
    "# test_dir = r\"/leonardo/home/userexternal/anagi000/Project/swin_transformer/training_data_split/val\"\n",
    "\n",
    "val_dir = r\"./ViT_Batch_Job/training_data_split_Conformal_Prediction/val\"\n",
    "test_dir = r\"./ViT_Batch_Job/training_data_split_Conformal_Prediction/test\"\n",
    "\n",
    "# Create image size (from Table 3 in the ViT paper)\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "from going_modular import data_setup\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "val_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=val_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_dataloader, test_dataloader, class_names\n",
    "\n",
    "\n",
    "from going_modular import data_setup\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "val_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=val_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_dataloader, test_dataloader, class_names\n",
    "\n",
    "\n",
    "class_names\n",
    "\n",
    "\n",
    "from going_modular.helper_functions import set_seeds\n",
    "set_seeds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed26376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07120f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fbde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10742f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cccdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get pretrained weights for ViT-Base\n",
    "# Get the pretrained weights for ViT-Base trained on ImageNet-1k from\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "#for parameter in pretrained_vit.parameters():\n",
    "#    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
    "#set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf3876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4d10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506678a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc572d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667b457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58fba4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alita\\AppData\\Local\\Temp\\ipykernel_10888\\935966064.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_vit.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(\"D:/VSCode_Projects/Thesis_Project/Experiments/ViT_Experiment_9-7-2025\") / \"models\" / \"model_best_epoch_92_acc_0.9860.pth\"\n",
    "#model_path = Path(output_dir) / \"models\" / \"Pretrained_vit__patches_sliding_window_algorithm_all_classes_scratch_training_mix_data_split_Skin_Disease_Training_Data_Random_Sampling.pth\"\n",
    "\n",
    "pretrained_vit.load_state_dict(torch.load(model_path, map_location=device))\n",
    "pretrained_vit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ef561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "\n",
    "#Original\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "\n",
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)\n",
    "\n",
    "\n",
    "\n",
    "# Setup dataloaders\n",
    "val_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=val_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32)\n",
    "\n",
    "from going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "#optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "#                             lr=1e-3)\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-5)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77dde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b10ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff576908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa458e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b7a6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56572db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model 0 results on test dataset\n",
    "# model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "#     loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    "# )\n",
    "# model_0_results\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "vit_eval_results = eval_model(\n",
    "    model=pretrained_vit,\n",
    "    data_loader=test_dataloader_pretrained,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0bd97f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'VisionTransformer',\n",
       " 'model_loss': 0.7227729558944702,\n",
       " 'model_acc': 89.29836345872008}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_eval_results"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5d219ccd",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating: 100%|| 2046/2046 [2:46:54<00:00,  4.89s/it]  \n",
      "Testing: 100%|| 2047/2047 [2:45:33<00:00,  4.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformal Prediction Results:\n",
      "  - Coverage: 0.899\n",
      "  - Avg. Prediction Set Size: 1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def conformal_predict(model, dataloader, calib_scores, alpha, device):\n",
    "    model.eval()\n",
    "    prediction_sets = []\n",
    "    true_labels = []\n",
    "\n",
    "    #q_threshold = np.quantile(calib_scores, 1 - alpha)\n",
    "\n",
    "     # Apply finite sample correction to the quantile level\n",
    "    number_of_calibration_samples = len(calib_scores)\n",
    "    #qlevel =         (1 - alpha) * ((number_of_calibration_samples + 1) / number_of_calibration_samples)\n",
    "    qlevel = np.ceil((1 - alpha) * (number_of_calibration_samples + 1)) / number_of_calibration_samples\n",
    "\n",
    "    \n",
    "    #q_threshold = np.quantile(calib_scores, qlevel) # Use qlevel directly as it's already a percentile (0-1)\n",
    "    q_threshold =  np.quantile(calib_scores, qlevel, interpolation=\"higher\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(probs.shape[0]):\n",
    "                # Prediction set = classes where 1 - prob <= qthreshold  prob >= 1 - qthreshold\n",
    "                pred_set = torch.where(probs[i] >= (1 - q_threshold))[0].cpu().tolist()\n",
    "                prediction_sets.append(pred_set)\n",
    "\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return prediction_sets, true_labels\n",
    "\n",
    "\n",
    "def evaluate_prediction_sets(prediction_sets, true_labels):\n",
    "    correct = 0\n",
    "    for pred_set, true_label in zip(prediction_sets, true_labels):\n",
    "        if true_label in pred_set:\n",
    "            correct += 1\n",
    "    coverage = correct / len(true_labels)\n",
    "    avg_set_size = np.mean([len(s) for s in prediction_sets])\n",
    "    return coverage, avg_set_size\n",
    "\n",
    "\n",
    "# Step 1: Load your model\n",
    "#model.load_state_dict(torch.load(\"path_to_model.pth\"))\n",
    "#model.to(device)\n",
    "\n",
    "# Step 2: Compute calibration scores\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)\n",
    "\n",
    "# Step 3: Choose confidence level (e.g., 90%)\n",
    "alpha = 0.1\n",
    "\n",
    "# Step 4: Make predictions on test set\n",
    "prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "coverage, avg_size = evaluate_prediction_sets(prediction_sets, true_labels)\n",
    "\n",
    "print(f\"Conformal Prediction Results:\")\n",
    "print(f\"  - Coverage: {coverage:.3f}\")\n",
    "print(f\"  - Avg. Prediction Set Size: {avg_size:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79944f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   3%|         | 52/2046 [04:30<2:41:51,  4.87s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def conformal_predict(model, dataloader, calib_scores, alpha, device):\n",
    "    model.eval()\n",
    "    prediction_sets = []\n",
    "    true_labels = []\n",
    "\n",
    "    #q_threshold = np.quantile(calib_scores, 1 - alpha)\n",
    "\n",
    "     # Apply finite sample correction to the quantile level\n",
    "    number_of_calibration_samples = len(calib_scores)\n",
    "    #qlevel =         (1 - alpha) * ((number_of_calibration_samples + 1) / number_of_calibration_samples)\n",
    "    qlevel = np.ceil((1 - alpha) * (number_of_calibration_samples + 1)) / number_of_calibration_samples\n",
    "\n",
    "    \n",
    "    #q_threshold = np.quantile(calib_scores, qlevel) # Use qlevel directly as it's already a percentile (0-1)\n",
    "    q_threshold =  np.quantile(calib_scores, qlevel, interpolation=\"higher\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(probs.shape[0]):\n",
    "                # Prediction set = classes where 1 - prob <= qthreshold  prob >= 1 - qthreshold\n",
    "                pred_set = torch.where(probs[i] >= (1 - q_threshold))[0].cpu().tolist()\n",
    "                prediction_sets.append(pred_set)\n",
    "\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return prediction_sets, true_labels\n",
    "\n",
    "\n",
    "def evaluate_prediction_sets(prediction_sets, true_labels):\n",
    "    correct = 0\n",
    "    for pred_set, true_label in zip(prediction_sets, true_labels):\n",
    "        if true_label in pred_set:\n",
    "            correct += 1\n",
    "    coverage = correct / len(true_labels)\n",
    "    avg_set_size = np.mean([len(sets) for sets in prediction_sets])\n",
    "    return coverage, avg_set_size\n",
    "\n",
    "\n",
    "# Step 1: Load your model\n",
    "#model.load_state_dict(torch.load(\"path_to_model.pth\"))\n",
    "#model.to(device)\n",
    "\n",
    "# Step 2: Compute calibration scores\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)\n",
    "\n",
    "# Step 3: Choose confidence level (e.g., 90%)\n",
    "alpha = 0.1\n",
    "\n",
    "# Step 4: Make predictions on test set\n",
    "prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "coverage, avg_size = evaluate_prediction_sets(prediction_sets, true_labels)\n",
    "\n",
    "print(f\"Conformal Prediction Results:\")\n",
    "print(f\"  - Coverage: {coverage:.3f}\")\n",
    "print(f\"  - Avg. Prediction Set Size: {avg_size:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc1eb396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 1/2046 [00:33<18:47:01, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.7946e-04, 1.0848e-05, 5.4836e-06, 3.2187e-06, 1.2875e-05, 7.0333e-06,\n",
      "        2.0266e-06, 1.5855e-05, 4.2915e-06, 2.2650e-06, 1.1086e-05, 3.3379e-06,\n",
      "        1.2720e-04, 9.9977e-01, 6.0797e-06, 9.2983e-06, 1.5855e-05, 6.7949e-06,\n",
      "        1.2636e-05, 1.1146e-04, 1.3113e-06, 2.8849e-01, 8.5831e-06, 1.9908e-05,\n",
      "        5.1022e-05, 9.9884e-01, 9.9983e-01, 6.7949e-06, 3.5763e-07, 9.3066e-01,\n",
      "        7.3910e-06, 4.6492e-06])\n",
      "[0.00047945976, 1.0848045e-05, 5.4836273e-06, 3.2186508e-06, 1.2874603e-05, 7.033348e-06, 2.026558e-06, 1.5854836e-05, 4.2915344e-06, 2.2649765e-06, 1.1086464e-05, 3.33786e-06, 0.00012719631, 0.99976593, 6.0796738e-06, 9.298325e-06, 1.5854836e-05, 6.7949295e-06, 1.2636185e-05, 0.000111460686, 1.3113022e-06, 0.2884916, 8.583069e-06, 1.9907951e-05, 5.1021576e-05, 0.9988442, 0.99982727, 6.7949295e-06, 3.5762787e-07, 0.9306628, 7.390976e-06, 4.6491623e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 2/2046 [00:37<9:05:01, 16.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1921e-05, 3.4213e-05, 2.1458e-06, 8.3588e-01, 3.2187e-06, 1.7762e-05,\n",
      "        3.8147e-06, 5.2214e-05, 4.9353e-05, 2.4756e-03, 1.0610e-05, 2.3246e-05,\n",
      "        7.7486e-06, 2.3842e-06, 8.8453e-05, 1.2517e-05, 1.5497e-05, 7.7033e-04,\n",
      "        1.5497e-05, 9.7017e-01, 5.7220e-06, 2.8610e-06, 1.9207e-01, 2.0266e-06,\n",
      "        4.7684e-06, 1.0729e-06, 1.7881e-05, 9.9449e-01, 1.6570e-05, 7.2718e-06,\n",
      "        4.6289e-04, 1.7285e-05])\n",
      "[0.00047945976, 1.0848045e-05, 5.4836273e-06, 3.2186508e-06, 1.2874603e-05, 7.033348e-06, 2.026558e-06, 1.5854836e-05, 4.2915344e-06, 2.2649765e-06, 1.1086464e-05, 3.33786e-06, 0.00012719631, 0.99976593, 6.0796738e-06, 9.298325e-06, 1.5854836e-05, 6.7949295e-06, 1.2636185e-05, 0.000111460686, 1.3113022e-06, 0.2884916, 8.583069e-06, 1.9907951e-05, 5.1021576e-05, 0.9988442, 0.99982727, 6.7949295e-06, 3.5762787e-07, 0.9306628, 7.390976e-06, 4.6491623e-06, 1.1920929e-05, 3.4213066e-05, 2.1457672e-06, 0.8358775, 3.2186508e-06, 1.7762184e-05, 3.8146973e-06, 5.221367e-05, 4.9352646e-05, 0.0024755597, 1.0609627e-05, 2.3245811e-05, 7.748604e-06, 2.3841858e-06, 8.845329e-05, 1.2516975e-05, 1.5497208e-05, 0.0007703304, 1.5497208e-05, 0.97016525, 5.722046e-06, 2.861023e-06, 0.1920734, 2.026558e-06, 4.7683716e-06, 1.0728836e-06, 1.7881393e-05, 0.9944891, 1.6570091e-05, 7.2717667e-06, 0.00046288967, 1.7285347e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 3/2046 [00:41<5:56:54, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0169e-04, 7.9870e-06, 5.4836e-05, 5.8655e-03, 2.4199e-05, 3.9339e-06,\n",
      "        1.4067e-05, 8.2254e-06, 1.1563e-05, 9.9996e-01, 4.0090e-01, 7.6294e-06,\n",
      "        1.0490e-05, 4.7684e-06, 4.3154e-05, 2.2650e-06, 1.0014e-05, 2.3842e-07,\n",
      "        7.8678e-06, 2.0266e-06, 3.2902e-05, 8.5147e-03, 8.7023e-06, 4.6492e-06,\n",
      "        5.0306e-05, 2.3007e-05, 1.0462e-01, 4.7684e-06, 3.6657e-04, 7.2718e-06,\n",
      "        4.5180e-05, 3.8147e-06])\n",
      "[0.00047945976, 1.0848045e-05, 5.4836273e-06, 3.2186508e-06, 1.2874603e-05, 7.033348e-06, 2.026558e-06, 1.5854836e-05, 4.2915344e-06, 2.2649765e-06, 1.1086464e-05, 3.33786e-06, 0.00012719631, 0.99976593, 6.0796738e-06, 9.298325e-06, 1.5854836e-05, 6.7949295e-06, 1.2636185e-05, 0.000111460686, 1.3113022e-06, 0.2884916, 8.583069e-06, 1.9907951e-05, 5.1021576e-05, 0.9988442, 0.99982727, 6.7949295e-06, 3.5762787e-07, 0.9306628, 7.390976e-06, 4.6491623e-06, 1.1920929e-05, 3.4213066e-05, 2.1457672e-06, 0.8358775, 3.2186508e-06, 1.7762184e-05, 3.8146973e-06, 5.221367e-05, 4.9352646e-05, 0.0024755597, 1.0609627e-05, 2.3245811e-05, 7.748604e-06, 2.3841858e-06, 8.845329e-05, 1.2516975e-05, 1.5497208e-05, 0.0007703304, 1.5497208e-05, 0.97016525, 5.722046e-06, 2.861023e-06, 0.1920734, 2.026558e-06, 4.7683716e-06, 1.0728836e-06, 1.7881393e-05, 0.9944891, 1.6570091e-05, 7.2717667e-06, 0.00046288967, 1.7285347e-05, 0.000101685524, 7.987022e-06, 5.4836273e-05, 0.0058655143, 2.4199486e-05, 3.9339066e-06, 1.4066696e-05, 8.225441e-06, 1.1563301e-05, 0.99996424, 0.40090227, 7.6293945e-06, 1.04904175e-05, 4.7683716e-06, 4.3153763e-05, 2.2649765e-06, 1.001358e-05, 2.3841858e-07, 7.867813e-06, 2.026558e-06, 3.2901764e-05, 0.008514702, 8.702278e-06, 4.6491623e-06, 5.030632e-05, 2.3007393e-05, 0.10461843, 4.7683716e-06, 0.00036656857, 7.2717667e-06, 4.518032e-05, 3.8146973e-06]\n",
      "tensor([1.3156e-02, 9.7752e-06, 5.3814e-01, 6.4373e-06, 1.4889e-04, 1.0729e-05,\n",
      "        5.1260e-06, 1.5259e-05, 6.9141e-06, 7.1526e-07, 8.3447e-07, 3.2783e-05,\n",
      "        4.3169e-01, 2.2179e-04, 3.4451e-05, 5.7220e-06, 2.1815e-05, 1.5855e-05,\n",
      "        2.8425e-04, 6.3300e-05, 5.3167e-05, 3.0875e-05, 5.9605e-07, 8.8215e-06,\n",
      "        1.2755e-05, 9.7144e-01, 1.6689e-05, 3.3140e-05, 6.5093e-02, 2.1458e-06,\n",
      "        8.9407e-06, 4.0293e-05])\n",
      "[0.00047945976, 1.0848045e-05, 5.4836273e-06, 3.2186508e-06, 1.2874603e-05, 7.033348e-06, 2.026558e-06, 1.5854836e-05, 4.2915344e-06, 2.2649765e-06, 1.1086464e-05, 3.33786e-06, 0.00012719631, 0.99976593, 6.0796738e-06, 9.298325e-06, 1.5854836e-05, 6.7949295e-06, 1.2636185e-05, 0.000111460686, 1.3113022e-06, 0.2884916, 8.583069e-06, 1.9907951e-05, 5.1021576e-05, 0.9988442, 0.99982727, 6.7949295e-06, 3.5762787e-07, 0.9306628, 7.390976e-06, 4.6491623e-06, 1.1920929e-05, 3.4213066e-05, 2.1457672e-06, 0.8358775, 3.2186508e-06, 1.7762184e-05, 3.8146973e-06, 5.221367e-05, 4.9352646e-05, 0.0024755597, 1.0609627e-05, 2.3245811e-05, 7.748604e-06, 2.3841858e-06, 8.845329e-05, 1.2516975e-05, 1.5497208e-05, 0.0007703304, 1.5497208e-05, 0.97016525, 5.722046e-06, 2.861023e-06, 0.1920734, 2.026558e-06, 4.7683716e-06, 1.0728836e-06, 1.7881393e-05, 0.9944891, 1.6570091e-05, 7.2717667e-06, 0.00046288967, 1.7285347e-05, 0.000101685524, 7.987022e-06, 5.4836273e-05, 0.0058655143, 2.4199486e-05, 3.9339066e-06, 1.4066696e-05, 8.225441e-06, 1.1563301e-05, 0.99996424, 0.40090227, 7.6293945e-06, 1.04904175e-05, 4.7683716e-06, 4.3153763e-05, 2.2649765e-06, 1.001358e-05, 2.3841858e-07, 7.867813e-06, 2.026558e-06, 3.2901764e-05, 0.008514702, 8.702278e-06, 4.6491623e-06, 5.030632e-05, 2.3007393e-05, 0.10461843, 4.7683716e-06, 0.00036656857, 7.2717667e-06, 4.518032e-05, 3.8146973e-06, 0.01315558, 9.775162e-06, 0.5381367, 6.4373016e-06, 0.0001488924, 1.0728836e-05, 5.1259995e-06, 1.5258789e-05, 6.914139e-06, 7.1525574e-07, 8.34465e-07, 3.2782555e-05, 0.43169367, 0.00022178888, 3.4451485e-05, 5.722046e-06, 2.18153e-05, 1.5854836e-05, 0.00028425455, 6.330013e-05, 5.3167343e-05, 3.0875206e-05, 5.9604645e-07, 8.821487e-06, 1.2755394e-05, 0.9714384, 1.66893e-05, 3.3140182e-05, 0.06509346, 2.1457672e-06, 8.940697e-06, 4.029274e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 3/2046 [00:46<8:42:11, 15.34s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    i = 0 \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            #print(logits)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            #print(probs)\n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            print(batch_scores)\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "            print(scores)\n",
    "            if i == 3:\n",
    "                break\n",
    "            i = i + 1\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_predict(model, dataloader, calib_scores, alpha, device):\n",
    "    model.eval()\n",
    "    prediction_sets = []\n",
    "    true_labels = []\n",
    "\n",
    "    #q_threshold = np.quantile(calib_scores, 1 - alpha)\n",
    "\n",
    "     # Apply finite sample correction to the quantile level\n",
    "    number_of_calibration_samples = len(calib_scores)\n",
    "    #qlevel =         (1 - alpha) * ((number_of_calibration_samples + 1) / number_of_calibration_samples)\n",
    "    qlevel = np.ceil((1 - alpha) * (number_of_calibration_samples + 1)) / number_of_calibration_samples\n",
    "\n",
    "    \n",
    "    #q_threshold = np.quantile(calib_scores, qlevel) # Use qlevel directly as it's already a percentile (0-1)\n",
    "    q_threshold =  np.quantile(calib_scores, qlevel, interpolation=\"higher\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(probs.shape[0]):\n",
    "                # Prediction set = classes where 1 - prob <= qthreshold  prob >= 1 - qthreshold\n",
    "                pred_set = torch.where(probs[i] >= (1 - q_threshold))[0].cpu().tolist()\n",
    "                prediction_sets.append(pred_set)\n",
    "\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return prediction_sets, true_labels\n",
    "\n",
    "\n",
    "def evaluate_prediction_sets(prediction_sets, true_labels):\n",
    "    correct = 0\n",
    "    for pred_set, true_label in zip(prediction_sets, true_labels):\n",
    "        if true_label in pred_set:\n",
    "            correct += 1\n",
    "    coverage = correct / len(true_labels)\n",
    "    avg_set_size = np.mean([len(s) for s in prediction_sets])\n",
    "    return coverage, avg_set_size\n",
    "\n",
    "\n",
    "# Step 1: Load your model\n",
    "#model.load_state_dict(torch.load(\"path_to_model.pth\"))\n",
    "#model.to(device)\n",
    "\n",
    "# Step 2: Compute calibration scores\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)\n",
    "\n",
    "# Step 3: Choose confidence level (e.g., 90%)\n",
    "alpha = 0.1\n",
    "\n",
    "# Step 4: Make predictions on test set\n",
    "prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fc30f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "probs = tensor([[6.8709e-08, 4.2351e-08, 1.2112e-07, 6.8505e-07, 1.0000e+00, 7.0042e-08,\n",
    "         2.2910e-08, 1.8036e-08, 1.1323e-08],\n",
    "        [6.7193e-07, 1.2221e-07, 6.8735e-07, 9.9998e-01, 3.8082e-07, 1.8035e-05,\n",
    "         6.2942e-07, 3.2710e-07, 4.7984e-07],\n",
    "        [1.7658e-07, 9.0674e-07, 5.5080e-08, 3.2845e-06, 1.4192e-07, 4.2425e-06,\n",
    "         4.4423e-07, 9.9999e-01, 7.4198e-07],\n",
    "        [3.7963e-06, 1.3325e-06, 1.1986e-05, 9.9996e-01, 2.3790e-06, 9.9154e-07,\n",
    "         2.3323e-06, 2.1640e-05, 2.4848e-07],\n",
    "        [1.0980e-06, 1.3851e-07, 5.1018e-07, 1.0000e+00, 5.4728e-07, 5.1938e-07,\n",
    "         3.8068e-07, 1.4570e-06, 1.1090e-07],\n",
    "        [2.2604e-07, 3.8123e-08, 1.0206e-07, 1.7244e-07, 7.3436e-08, 1.2486e-07,\n",
    "         1.7878e-08, 2.6392e-06, 1.0000e+00],\n",
    "        [6.1236e-07, 1.0261e-06, 8.2183e-07, 9.9999e-01, 1.1760e-06, 3.1296e-06,\n",
    "         1.8257e-06, 1.3217e-06, 1.4596e-07],\n",
    "        [1.1490e-06, 5.2309e-06, 3.2716e-06, 9.9998e-01, 1.0567e-05, 1.3835e-06,\n",
    "         1.1943e-06, 9.1565e-07, 1.0717e-07],\n",
    "        [2.1558e-07, 7.1635e-07, 4.9147e-08, 6.7290e-07, 1.0000e+00, 2.1276e-07,\n",
    "         1.4643e-07, 2.7387e-07, 1.5886e-07],\n",
    "        [1.4793e-05, 6.3944e-07, 5.0585e-07, 1.3170e-06, 6.3901e-07, 9.9998e-01,\n",
    "         8.9538e-07, 1.6281e-06, 1.1708e-06],\n",
    "        [1.1026e-07, 1.6902e-08, 1.3941e-07, 3.1041e-07, 1.0000e+00, 1.0697e-07,\n",
    "         4.2670e-08, 7.0726e-09, 2.1578e-07],\n",
    "        [1.5851e-07, 2.4890e-07, 6.2480e-07, 1.0000e+00, 6.6868e-07, 1.1450e-06,\n",
    "         4.1638e-07, 5.1002e-07, 1.0453e-07],\n",
    "        [5.0993e-07, 5.2527e-07, 8.0983e-07, 1.0000e+00, 2.3271e-07, 1.1941e-06,\n",
    "         5.3386e-07, 5.3900e-07, 1.5708e-07],\n",
    "        [2.7827e-06, 5.6192e-07, 2.9106e-07, 9.9999e-01, 1.8997e-07, 1.0677e-06,\n",
    "         1.0830e-06, 9.7273e-07, 2.9486e-07],\n",
    "        [9.4970e-01, 2.1365e-05, 2.0029e-05, 7.5098e-05, 8.9124e-06, 4.2412e-02,\n",
    "         3.6700e-05, 1.1299e-03, 6.5941e-03],\n",
    "        [1.4344e-06, 3.3623e-07, 1.4869e-07, 1.2360e-06, 2.8590e-06, 9.9999e-01,\n",
    "         1.0046e-06, 3.0356e-07, 1.1256e-06],\n",
    "        [2.0695e-06, 1.7753e-06, 3.3843e-06, 3.6050e-04, 7.2572e-07, 9.9826e-01,\n",
    "         4.9540e-05, 1.3243e-03, 6.2402e-07],\n",
    "        [5.7377e-07, 6.8458e-07, 9.9946e-01, 1.5731e-04, 2.5690e-07, 3.6886e-04,\n",
    "         7.4110e-07, 8.1294e-06, 1.1210e-06],\n",
    "        [9.9305e-06, 1.4071e-07, 1.7751e-07, 3.2548e-06, 1.1736e-06, 9.9998e-01,\n",
    "         4.4028e-07, 5.1738e-07, 6.4045e-08],\n",
    "        [3.5548e-04, 6.4539e-07, 2.1702e-06, 9.1601e-01, 3.6517e-05, 1.0209e-03,\n",
    "         1.2837e-05, 1.5881e-04, 8.2401e-02],\n",
    "        [1.8818e-06, 2.3329e-06, 9.3004e-07, 1.7614e-06, 4.2815e-07, 1.3229e-05,\n",
    "         1.4555e-05, 9.9996e-01, 8.5313e-07],\n",
    "        [8.7667e-07, 9.5353e-07, 3.1144e-06, 9.9928e-01, 7.1372e-07, 7.0792e-04,\n",
    "         1.2871e-06, 7.4206e-06, 2.4123e-07],\n",
    "        [1.6697e-07, 1.1383e-07, 9.2113e-08, 5.4342e-07, 1.0000e+00, 2.7918e-08,\n",
    "         1.4185e-07, 1.3478e-07, 1.4846e-08],\n",
    "        [3.1294e-06, 4.0982e-07, 1.2234e-06, 9.9998e-01, 1.7090e-06, 4.9379e-06,\n",
    "         1.7861e-06, 3.3276e-06, 1.5979e-06],\n",
    "        [4.6791e-05, 7.8141e-06, 3.7183e-05, 9.0011e-01, 1.3935e-06, 8.1496e-02,\n",
    "         2.0576e-05, 1.8025e-02, 2.5535e-04],\n",
    "        [1.2839e-07, 3.1351e-07, 1.1039e-06, 3.8231e-07, 1.0000e+00, 9.8887e-08,\n",
    "         9.2726e-08, 4.5573e-08, 5.0211e-08],\n",
    "        [1.3837e-07, 2.2378e-07, 3.0250e-07, 6.5579e-07, 1.0000e+00, 8.4995e-08,\n",
    "         4.8761e-08, 3.9466e-08, 6.9660e-07],\n",
    "        [2.8623e-06, 3.7181e-07, 2.2368e-07, 2.2280e-04, 1.0319e-06, 2.2255e-06,\n",
    "         7.2536e-07, 2.3705e-07, 9.9977e-01],\n",
    "        [1.6225e-05, 2.6943e-06, 6.5512e-07, 9.9989e-01, 1.2256e-06, 8.5345e-05,\n",
    "         3.8037e-06, 3.3679e-06, 2.6572e-07],\n",
    "        [7.3648e-06, 1.0283e-06, 3.6701e-06, 9.9984e-01, 1.1412e-04, 1.4009e-05,\n",
    "         6.3348e-06, 1.1067e-05, 3.4393e-06],\n",
    "        [1.7725e-06, 3.8951e-07, 2.9109e-06, 9.9999e-01, 3.9279e-06, 1.0808e-06,\n",
    "         6.9467e-07, 1.0790e-06, 8.2957e-08],\n",
    "        [3.2619e-05, 3.4069e-06, 1.4480e-05, 1.9212e-03, 6.3451e-07, 9.9800e-01,\n",
    "         3.8464e-06, 1.0359e-05, 1.5819e-05]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c08bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 0/2046 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 6, 5, 0, 5, 5, 5, 8, 4, 0, 6, 6, 7, 7, 5, 5, 5, 7, 5, 2, 5, 4, 8,\n",
       "        5, 6, 3, 5, 4, 5, 3, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []\n",
    "for images, labels in tqdm(val_dataloader_pretrained, desc=\"Calibrating\"):\n",
    "    label = labels\n",
    "    break\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "009add70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db3b0de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0042e-08, 1.8035e-05, 4.4423e-07, 9.9154e-07, 1.0980e-06, 1.2486e-07,\n",
       "        3.1296e-06, 1.3835e-06, 1.5886e-07, 6.3901e-07, 1.1026e-07, 4.1638e-07,\n",
       "        5.3386e-07, 9.7273e-07, 1.1299e-03, 9.9999e-01, 9.9826e-01, 3.6886e-04,\n",
       "        5.1738e-07, 1.0209e-03, 9.3004e-07, 7.0792e-04, 1.0000e+00, 1.5979e-06,\n",
       "        8.1496e-02, 9.2726e-08, 6.5579e-07, 2.2255e-06, 1.2256e-06, 1.4009e-05,\n",
       "        9.9999e-01, 9.9800e-01])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(len(labels)), labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12c49e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.9998e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 9.9887e-01, 1.0014e-05, 1.7400e-03, 9.9963e-01,\n",
       "        1.0000e+00, 9.9898e-01, 1.0000e+00, 9.9929e-01, 0.0000e+00, 1.0000e+00,\n",
       "        9.1850e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
       "        1.0014e-05, 2.0000e-03])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probs[torch.arange(len(labels)), labels] = tensor([0.7, 0.6])\n",
    "batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "batch_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8243c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[torch.arange(2), labels] = tensor([0.7, 0.6])\n",
    "batch_scores = 1 - tensor([0.7, 0.6]) = tensor([0.3, 0.4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bf49896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99999994,\n",
       " 0.99998194,\n",
       " 0.9999996,\n",
       " 0.999999,\n",
       " 0.9999989,\n",
       " 0.9999999,\n",
       " 0.99999684,\n",
       " 0.9999986,\n",
       " 0.9999998,\n",
       " 0.99999934,\n",
       " 0.9999999,\n",
       " 0.9999996,\n",
       " 0.99999946,\n",
       " 0.99999905,\n",
       " 0.9988701,\n",
       " 1.001358e-05,\n",
       " 0.0017399788,\n",
       " 0.99963117,\n",
       " 0.99999946,\n",
       " 0.9989791,\n",
       " 0.99999905,\n",
       " 0.9992921,\n",
       " 0.0,\n",
       " 0.9999984,\n",
       " 0.918504,\n",
       " 0.9999999,\n",
       " 0.99999934,\n",
       " 0.9999978,\n",
       " 0.99999875,\n",
       " 0.999986,\n",
       " 1.001358e-05,\n",
       " 0.0019999743]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "scores.extend(batch_scores.cpu().numpy())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e972198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    i = 0 \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            print(logits)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            print(probs)\n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            print(batch_scores)\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "            print(scores)\n",
    "            #if i == 3:\n",
    "            break\n",
    "            #i = i + 1\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3aeea7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:   0%|          | 0/2046 [00:34<?, ?it/s]\n",
      "C:\\Users\\alita\\AppData\\Local\\Temp\\ipykernel_10888\\380311795.py:74: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of calibration samples:  32\n",
      "qlevel of samples:  0.9375\n",
      "q threshold of samples:  0.09103668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/2047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits :  tensor([[ 5.9199e+00, -2.7402e+00, -4.1140e+00,  8.2474e+00,  1.1109e+00,\n",
      "         -3.0968e+00, -3.4346e+00, -2.2069e+00, -3.8494e+00],\n",
      "        [ 7.7791e+00, -5.0009e+00, -3.3917e+00,  4.7646e+00,  2.0988e+00,\n",
      "         -1.9518e+00, -4.0907e+00, -1.4292e+00, -4.3785e+00],\n",
      "        [ 7.5201e+00, -2.7193e+00, -4.9081e+00,  7.3716e+00, -5.7079e-01,\n",
      "         -3.5404e+00, -3.7196e+00, -1.2865e+00, -2.5679e+00],\n",
      "        [ 1.5246e+01, -1.6633e+00, -2.6756e+00, -1.2675e+00, -7.3678e-01,\n",
      "         -1.2178e+00, -1.6864e+00, -1.0405e+00, -2.6251e+00],\n",
      "        [ 1.5176e+01, -9.1834e-01, -2.5375e+00,  1.5291e-01, -1.1559e+00,\n",
      "         -3.4724e+00, -1.4233e+00,  8.7237e-02, -3.0716e+00],\n",
      "        [ 1.5545e+01, -1.5590e+00, -2.4437e+00, -5.2961e-01, -1.2884e+00,\n",
      "         -2.3407e+00, -1.6094e+00, -9.4047e-01, -2.3884e+00],\n",
      "        [ 1.4938e+01, -3.1650e+00, -3.7853e+00,  3.1453e-01, -1.7199e+00,\n",
      "         -1.0008e+00, -1.2574e+00, -8.4869e-01, -2.5367e+00],\n",
      "        [ 1.4797e+01, -1.1838e+00, -2.7418e+00,  3.5450e-01, -1.3851e+00,\n",
      "         -3.1550e+00, -2.3287e+00, -2.8422e-02, -2.8229e+00],\n",
      "        [ 1.5309e+01, -1.4360e+00, -2.8806e+00,  5.3726e-01, -1.3781e+00,\n",
      "         -2.8645e+00, -1.8063e+00, -8.9119e-01, -2.9139e+00],\n",
      "        [ 9.0335e+00, -2.2677e+00, -3.4054e+00,  6.2856e+00, -1.6628e+00,\n",
      "         -3.2718e+00, -1.9046e+00, -1.6178e+00, -5.7625e+00],\n",
      "        [ 1.3418e+01, -2.3316e+00, -4.1836e+00,  3.1328e+00, -1.9794e+00,\n",
      "         -2.6742e+00, -2.2494e+00, -8.9737e-01, -3.7584e+00],\n",
      "        [ 9.2393e+00,  1.1729e+00, -2.3926e+00, -1.0175e+00, -1.8962e+00,\n",
      "          1.0711e+00, -2.8495e+00, -5.8608e-01, -7.9081e-01],\n",
      "        [ 8.4875e+00,  8.6136e-01, -3.2921e+00, -8.2744e-01, -2.4047e+00,\n",
      "          3.4832e+00, -2.6529e+00, -1.5201e-01, -3.8081e+00],\n",
      "        [ 1.0627e+00, -2.6767e+00, -2.6083e+00,  1.1195e+01, -2.9604e+00,\n",
      "         -3.0767e+00, -2.9557e+00, -2.7171e+00, -4.2815e+00],\n",
      "        [ 4.8427e+00, -3.5695e+00, -5.7035e-01,  7.8013e+00, -2.7638e+00,\n",
      "         -9.7507e-01, -3.5883e+00, -3.4866e+00, -5.8162e+00],\n",
      "        [ 3.4876e+00, -3.2590e+00, -2.4212e+00,  9.8998e+00, -2.6204e+00,\n",
      "         -2.8190e+00, -3.0598e+00, -2.9000e+00, -4.6781e+00],\n",
      "        [ 5.5814e+00, -2.1839e+00, -2.8544e+00,  8.9823e+00, -3.6989e+00,\n",
      "         -3.0699e+00, -2.8633e+00, -2.5251e+00, -4.1996e+00],\n",
      "        [ 8.8100e+00, -2.2134e+00, -2.9348e+00,  7.0154e+00, -3.2315e+00,\n",
      "         -4.1716e+00, -3.0921e+00, -2.9727e+00, -1.5688e+00],\n",
      "        [ 6.3498e+00, -2.4243e+00, -1.9571e+00,  2.7878e+00, -3.2749e+00,\n",
      "          2.9650e+00, -2.6059e+00, -9.6977e-01, -4.2160e+00],\n",
      "        [ 1.4953e+01, -2.8873e+00, -3.7257e+00, -6.5909e-01, -8.7775e-01,\n",
      "         -1.7753e+00, -1.0730e+00, -1.5262e-01, -1.3562e+00],\n",
      "        [ 1.4203e+01, -2.4384e+00, -1.9473e+00, -1.4118e+00, -7.5998e-01,\n",
      "         -1.0038e+00, -1.6964e+00, -1.6357e+00, -1.6540e+00],\n",
      "        [ 1.4800e+01, -3.0851e+00, -3.5758e+00, -9.1910e-01, -7.9313e-01,\n",
      "         -1.5410e+00, -1.3583e+00, -4.1765e-01, -1.2470e+00],\n",
      "        [ 1.4542e+01, -3.2893e+00, -3.0170e+00,  1.7193e-01, -3.6098e-01,\n",
      "         -1.2758e+00, -8.5459e-01, -1.2825e+00, -3.7296e+00],\n",
      "        [ 1.4460e+01, -2.7448e+00, -1.7795e+00, -1.5175e+00, -2.5333e-02,\n",
      "         -1.1691e+00, -1.2485e+00, -1.2278e+00, -2.2661e+00],\n",
      "        [ 1.4452e+01, -2.1367e+00, -1.3223e+00, -1.5773e+00,  4.0324e-02,\n",
      "         -1.7534e+00, -1.4325e+00, -6.6588e-01, -2.7201e+00],\n",
      "        [ 1.3486e+01, -3.5074e+00, -3.8838e+00, -2.0798e-02, -1.3356e+00,\n",
      "          4.8191e-02, -2.0943e+00, -9.8213e-01, -2.4861e+00],\n",
      "        [ 1.4782e+01, -3.0026e+00, -3.5993e+00, -8.7917e-01, -1.6114e+00,\n",
      "          6.2164e-03, -2.0179e+00, -7.3019e-01, -2.3754e+00],\n",
      "        [ 1.5155e+01, -1.6584e+00, -3.0959e+00, -8.2816e-01, -8.8489e-01,\n",
      "         -2.0616e+00, -1.8938e+00, -8.7516e-01, -1.4013e+00],\n",
      "        [ 1.4305e+01, -2.8585e+00, -2.6172e+00, -5.8048e-01, -7.9072e-01,\n",
      "         -1.3437e+00, -9.8813e-01, -1.1145e+00, -2.3876e+00],\n",
      "        [ 1.3812e+01, -2.5727e+00, -3.9083e+00,  9.3847e-01, -1.4619e+00,\n",
      "         -1.9935e+00, -1.9461e+00, -5.7730e-02, -2.7332e+00],\n",
      "        [ 1.3851e+01, -2.8307e+00, -3.1423e+00, -6.0747e-01, -1.5862e+00,\n",
      "         -3.8454e-01, -2.2135e+00, -1.5377e+00, -1.4767e+00],\n",
      "        [ 1.2171e+01, -4.0025e+00, -4.3339e+00, -2.8057e-01, -1.5698e+00,\n",
      "          8.6034e-01, -2.5040e+00,  7.2617e-01, -2.4588e+00]])\n",
      "probabilities :  tensor([[8.8798e-02, 1.5394e-05, 3.8968e-06, 9.1041e-01, 7.2423e-04, 1.0777e-05,\n",
      "         7.6873e-06, 2.6239e-05, 5.0775e-06],\n",
      "        [9.4997e-01, 2.6754e-06, 1.3375e-05, 4.6612e-02, 3.2418e-03, 5.6444e-05,\n",
      "         6.6480e-06, 9.5188e-05, 4.9853e-06],\n",
      "        [5.3688e-01, 1.9186e-05, 2.1497e-06, 4.6281e-01, 1.6446e-04, 8.4406e-06,\n",
      "         7.0560e-06, 8.0396e-05, 2.2323e-05],\n",
      "        [1.0000e+00, 4.5346e-08, 1.6477e-08, 6.7362e-08, 1.1453e-07, 7.0797e-08,\n",
      "         4.4309e-08, 8.4533e-08, 1.7330e-08],\n",
      "        [1.0000e+00, 1.0242e-07, 2.0286e-08, 2.9897e-07, 8.0769e-08, 7.9645e-09,\n",
      "         6.1815e-08, 2.7997e-07, 1.1892e-08],\n",
      "        [1.0000e+00, 3.7290e-08, 1.5395e-08, 1.0439e-07, 4.8879e-08, 1.7065e-08,\n",
      "         3.5458e-08, 6.9222e-08, 1.6271e-08],\n",
      "        [1.0000e+00, 1.3741e-08, 7.3899e-09, 4.4583e-07, 5.8297e-08, 1.1966e-07,\n",
      "         9.2575e-08, 1.3931e-07, 2.5757e-08],\n",
      "        [1.0000e+00, 1.1466e-07, 2.4143e-08, 5.3395e-07, 9.3757e-08, 1.5971e-08,\n",
      "         3.6493e-08, 3.6408e-07, 2.2262e-08],\n",
      "        [1.0000e+00, 5.3423e-08, 1.2600e-08, 3.8433e-07, 5.6607e-08, 1.2804e-08,\n",
      "         3.6891e-08, 9.2117e-08, 1.2187e-08],\n",
      "        [9.3972e-01, 1.1613e-05, 3.7227e-06, 6.0199e-02, 2.1263e-05, 4.2547e-06,\n",
      "         1.6697e-05, 2.2242e-05, 3.5253e-07],\n",
      "        [9.9996e-01, 1.4458e-07, 2.2686e-08, 3.4139e-05, 2.0561e-07, 1.0264e-07,\n",
      "         1.5696e-07, 6.0670e-07, 3.4710e-08],\n",
      "        [9.9924e-01, 3.1369e-04, 8.8716e-06, 3.5092e-05, 1.4575e-05, 2.8331e-04,\n",
      "         5.6181e-06, 5.4021e-05, 4.4020e-05],\n",
      "        [9.9255e-01, 4.8389e-04, 7.6020e-06, 8.9393e-05, 1.8464e-05, 6.6586e-03,\n",
      "         1.4405e-05, 1.7565e-04, 4.5377e-06],\n",
      "        [3.9781e-05, 9.4551e-07, 1.0125e-06, 9.9996e-01, 7.1193e-07, 6.3375e-07,\n",
      "         7.1532e-07, 9.0809e-07, 1.8998e-07],\n",
      "        [4.9309e-02, 1.0953e-05, 2.1982e-04, 9.5027e-01, 2.4518e-05, 1.4666e-04,\n",
      "         1.0749e-05, 1.1900e-05, 1.1583e-06],\n",
      "        [1.6387e-03, 1.9252e-06, 4.4495e-06, 9.9834e-01, 3.6459e-06, 2.9893e-06,\n",
      "         2.3496e-06, 2.7566e-06, 4.6576e-07],\n",
      "        [3.2264e-02, 1.3686e-05, 7.0002e-06, 9.6769e-01, 3.0084e-06, 5.6428e-06,\n",
      "         6.9380e-06, 9.7297e-06, 1.8234e-06],\n",
      "        [8.5742e-01, 1.3989e-05, 6.7997e-06, 1.4251e-01, 5.0543e-06, 1.9741e-06,\n",
      "         5.8102e-06, 6.5472e-06, 2.6654e-05],\n",
      "        [9.4025e-01, 1.4545e-04, 2.3205e-04, 2.6686e-02, 6.2127e-05, 3.1858e-02,\n",
      "         1.2129e-04, 6.2285e-04, 2.4242e-05],\n",
      "        [1.0000e+00, 1.7872e-08, 7.7280e-09, 1.6591e-07, 1.3333e-07, 5.4337e-08,\n",
      "         1.0968e-07, 2.7532e-07, 8.2632e-08],\n",
      "        [1.0000e+00, 5.9231e-08, 9.6784e-08, 1.6534e-07, 3.1730e-07, 2.4865e-07,\n",
      "         1.2439e-07, 1.3217e-07, 1.2978e-07],\n",
      "        [1.0000e+00, 1.7082e-08, 1.0458e-08, 1.4901e-07, 1.6901e-07, 8.0005e-08,\n",
      "         9.6039e-08, 2.4603e-07, 1.0735e-07],\n",
      "        [1.0000e+00, 1.8026e-08, 2.3669e-08, 5.7426e-07, 3.3703e-07, 1.3502e-07,\n",
      "         2.0573e-07, 1.3411e-07, 1.1607e-08],\n",
      "        [1.0000e+00, 3.3721e-08, 8.8536e-08, 1.1505e-07, 5.1161e-07, 1.6300e-07,\n",
      "         1.5057e-07, 1.5371e-07, 5.4421e-08],\n",
      "        [1.0000e+00, 6.2438e-08, 1.4097e-07, 1.0924e-07, 5.5069e-07, 9.1602e-08,\n",
      "         1.2626e-07, 2.7177e-07, 3.4840e-08],\n",
      "        [1.0000e+00, 4.1663e-08, 2.8594e-08, 1.3613e-06, 3.6556e-07, 1.4586e-06,\n",
      "         1.7119e-07, 5.2056e-07, 1.1570e-07],\n",
      "        [1.0000e+00, 1.8895e-08, 1.0404e-08, 1.5796e-07, 7.5954e-08, 3.8287e-07,\n",
      "         5.0580e-08, 1.8333e-07, 3.5378e-08],\n",
      "        [1.0000e+00, 4.9886e-08, 1.1849e-08, 1.1443e-07, 1.0812e-07, 3.3331e-08,\n",
      "         3.9422e-08, 1.0917e-07, 6.4510e-08],\n",
      "        [1.0000e+00, 3.5142e-08, 4.4731e-08, 3.4289e-07, 2.7787e-07, 1.5983e-07,\n",
      "         2.2809e-07, 2.0101e-07, 5.6279e-08],\n",
      "        [1.0000e+00, 7.6576e-08, 2.0139e-08, 2.5643e-06, 2.3255e-07, 1.3666e-07,\n",
      "         1.4329e-07, 9.4693e-07, 6.5220e-08],\n",
      "        [1.0000e+00, 5.6891e-08, 4.1657e-08, 5.2550e-07, 1.9747e-07, 6.5674e-07,\n",
      "         1.0546e-07, 2.0729e-07, 2.2033e-07],\n",
      "        [9.9997e-01, 9.4589e-08, 6.7904e-08, 3.9106e-06, 1.0773e-06, 1.2239e-05,\n",
      "         4.2327e-07, 1.0702e-05, 4.4283e-07]])\n",
      "probabilities shape :  32\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([3]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([3])\n",
      "Single prediction set for a batch :  [3]\n",
      "Prediction sets :  [[3]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([], dtype=torch.int64),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([], dtype=torch.int64)\n",
      "Single prediction set for a batch :  []\n",
      "Prediction sets :  [[3], [0], []]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([3]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([3])\n",
      "Single prediction set for a batch :  [3]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([3]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([3])\n",
      "Single prediction set for a batch :  [3]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([3]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([3])\n",
      "Single prediction set for a batch :  [3]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([3]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([3])\n",
      "Single prediction set for a batch :  [3]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([], dtype=torch.int64),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([], dtype=torch.int64)\n",
      "Single prediction set for a batch :  []\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], []]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold)):  (tensor([0]),)\n",
      " torch.where torch.where(probs[i] >= (1 - q_threshold))[0]:  tensor([0])\n",
      "Single prediction set for a batch :  [0]\n",
      "Prediction sets :  [[3], [0], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/2047 [00:34<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "            break\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def conformal_predict(model, dataloader, calib_scores, alpha, device):\n",
    "    model.eval()\n",
    "    prediction_sets = []\n",
    "    true_labels = []\n",
    "\n",
    "    #q_threshold = np.quantile(calib_scores, 1 - alpha)\n",
    "\n",
    "     # Apply finite sample correction to the quantile level\n",
    "    number_of_calibration_samples = len(calib_scores)\n",
    "    print(\"Number of calibration samples: \", number_of_calibration_samples)\n",
    "    #qlevel =         (1 - alpha) * ((number_of_calibration_samples + 1) / number_of_calibration_samples)\n",
    "    qlevel = np.ceil((1 - alpha) * (number_of_calibration_samples + 1)) / number_of_calibration_samples\n",
    "    print(\"qlevel of samples: \", qlevel)\n",
    "    #q_threshold = np.quantile(calib_scores, qlevel) # Use qlevel directly as it's already a percentile (0-1)\n",
    "    q_threshold =  np.quantile(calib_scores, qlevel, interpolation=\"higher\")\n",
    "    print(\"q threshold of samples: \", q_threshold)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            print(\"logits : \", logits)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            print(\"probabilities : \",probs)\n",
    "            print(\"probabilities shape : \", (probs.shape[0]))\n",
    "            for i in range(probs.shape[0]):\n",
    "                # Prediction set = classes where 1 - prob <= qthreshold  prob >= 1 - qthreshold\n",
    "                print(\" torch.where torch.where(probs[i] >= (1 - q_threshold)): \", torch.where(probs[i] >= (1 - q_threshold)))\n",
    "                print(\" torch.where torch.where(probs[i] >= (1 - q_threshold))[0]: \", torch.where(probs[i] >= (1 - q_threshold))[0])\n",
    "                pred_set = torch.where(probs[i] >= (1 - q_threshold))[0].cpu().tolist()\n",
    "                print(\"Single prediction set for a batch : \", pred_set)\n",
    "                prediction_sets.append(pred_set)\n",
    "                print(\"Prediction sets : \", prediction_sets)\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "            print(true_labels)\n",
    "            break\n",
    "    return prediction_sets, true_labels\n",
    "\n",
    "\n",
    "# Step 1: Load your model\n",
    "#model.load_state_dict(torch.load(\"path_to_model.pth\"))\n",
    "#model.to(device)\n",
    "\n",
    "# Step 2: Compute calibration scores\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)\n",
    "\n",
    "# Step 3: Choose confidence level (e.g., 90%)\n",
    "alpha = 0.1\n",
    "\n",
    "# Step 4: Make predictions on test set\n",
    "prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "#coverage, avg_size = evaluate_prediction_sets(prediction_sets, true_labels)\n",
    "\n",
    "#print(f\"Conformal Prediction Results:\")\n",
    "#print(f\"  - Coverage: {coverage:.3f}\")\n",
    "#print(f\"  - Avg. Prediction Set Size: {avg_size:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_nonconformity_scores(model, dataloader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Calibrating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Nonconformity = 1 - probability of the true class\n",
    "            batch_scores = 1 - probs[torch.arange(len(labels)), labels]\n",
    "            scores.extend(batch_scores.cpu().numpy())\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def conformal_predict(model, dataloader, calib_scores, alpha, device):\n",
    "    model.eval()\n",
    "    prediction_sets = []\n",
    "    true_labels = []\n",
    "\n",
    "    #q_threshold = np.quantile(calib_scores, 1 - alpha)\n",
    "\n",
    "     # Apply finite sample correction to the quantile level\n",
    "    number_of_calibration_samples = len(calib_scores)\n",
    "    #qlevel =         (1 - alpha) * ((number_of_calibration_samples + 1) / number_of_calibration_samples)\n",
    "    qlevel = np.ceil((1 - alpha) * (number_of_calibration_samples + 1)) / number_of_calibration_samples\n",
    "\n",
    "    \n",
    "    #q_threshold = np.quantile(calib_scores, qlevel) # Use qlevel directly as it's already a percentile (0-1)\n",
    "    q_threshold =  np.quantile(calib_scores, qlevel, interpolation=\"higher\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(probs.shape[0]):\n",
    "                # Prediction set = classes where 1 - prob <= qthreshold  prob >= 1 - qthreshold\n",
    "                pred_set = torch.where(probs[i] >= (1 - q_threshold))[0].cpu().tolist()\n",
    "                prediction_sets.append(pred_set)\n",
    "\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return prediction_sets, true_labels\n",
    "\n",
    "\n",
    "def evaluate_prediction_sets(prediction_sets, true_labels):\n",
    "    correct = 0\n",
    "    for pred_set, true_label in zip(prediction_sets, true_labels):\n",
    "        if true_label in pred_set:\n",
    "            correct += 1\n",
    "    coverage = correct / len(true_labels)\n",
    "    avg_set_size = np.mean([len(set) for set in prediction_sets])\n",
    "    return coverage, avg_set_size\n",
    "\n",
    "\n",
    "# Step 1: Load your model\n",
    "#model.load_state_dict(torch.load(\"path_to_model.pth\"))\n",
    "#model.to(device)\n",
    "\n",
    "# Step 2: Compute calibration scores\n",
    "calib_scores = compute_nonconformity_scores(pretrained_vit, val_dataloader_pretrained, device)\n",
    "\n",
    "# Step 3: Choose confidence level (e.g., 90%)\n",
    "alpha = 0.1\n",
    "\n",
    "# Step 4: Make predictions on test set\n",
    "prediction_sets, true_labels = conformal_predict(pretrained_vit, test_dataloader_pretrained, calib_scores, alpha, device)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "coverage, avg_size = evaluate_prediction_sets(prediction_sets, true_labels)\n",
    "\n",
    "print(f\"Conformal Prediction Results:\")\n",
    "print(f\"  - Coverage: {coverage:.3f}\")\n",
    "print(f\"  - Avg. Prediction Set Size: {avg_size:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
